"""
Main Feed Generator Script for Production - MEMORY OPTIMIZED
Handles large stores (1000+ products) within 512MB RAM limit
"""

import os
import sys
import logging
import gc
from datetime import datetime, timezone
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('feed_generation.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Import modules
from src.shopify_client import ShopifyClient
from src.transformer import ProductTransformer
from src.xml_generator import StreamingXMLGenerator
from src.config_loader import ConfigLoader


class FeedGeneratorService:
    def __init__(self):
        # Load environment variables
        self.shop_url = os.getenv('SHOPIFY_SHOP_URL')
        self.access_token = os.getenv('SHOPIFY_ACCESS_TOKEN')
        self.base_url = os.getenv('SHOP_BASE_URL', 'https://racoon-lab.it')
        
        if not self.shop_url or not self.access_token:
            raise ValueError("Missing required environment variables: SHOPIFY_SHOP_URL, SHOPIFY_ACCESS_TOKEN")
        
        # Initialize components
        self.client = ShopifyClient(self.shop_url, self.access_token)
        self.config = ConfigLoader('config')
        self.transformer = ProductTransformer(self.config, self.base_url)
        
        # Output directory
        self.output_dir = Path('public')
        self.output_dir.mkdir(exist_ok=True)
        
    def generate_feed(self):
        """Generate complete Google Shopping feed with memory optimization"""
        try:
            logger.info("=" * 70)
            logger.info("üöÄ FEED GENERATION STARTED (MEMORY OPTIMIZED)")
            logger.info("=" * 70)
            logger.info(f"Time: {datetime.now(timezone.utc).isoformat()}")
            logger.info(f"Shop: {self.shop_url}")
            logger.info("")
            
            # Test connection
            logger.info("Testing Shopify connection...")
            if not self.client.test_connection():
                raise Exception("Failed to connect to Shopify API")
            
            # Fetch products WITHOUT metafields (saves memory)
            logger.info("Fetching products (without metafields)...")
            products = self.client.get_products(
                limit=250,
                fields='id,title,handle,body_html,vendor,product_type,tags,variants,images'
            )
            
            logger.info(f"Retrieved {len(products)} products")
            
            # Initialize streaming XML generator
            feed_path = self.output_dir / 'google_shopping_feed.xml'
            shop_info = {
                'title': 'Racoon Lab - Sneakers Personalizzate',
                'url': self.base_url
            }
            
            generator = StreamingXMLGenerator(str(feed_path), shop_info)
            
            # Process in batches to save memory
            BATCH_SIZE = 100
            total_items = 0
            products_with_reviews = 0
            
            logger.info("Processing products in batches...")
            
            for batch_start in range(0, len(products), BATCH_SIZE):
                batch_end = min(batch_start + BATCH_SIZE, len(products))
                batch = products[batch_start:batch_end]
                
                logger.info(f"Processing batch {batch_start}-{batch_end} ({len(batch)} products)...")
                
                # Process batch
                for product in batch:
                    # For memory optimization: only fetch metafields if product has reviews indicator
                    # Check if product might have reviews (based on tags or other indicators)
                    might_have_reviews = self._might_have_reviews(product)
                    
                    if might_have_reviews:
                        # Fetch metafields only for this product
                        metafields_data = self.client.get_product_metafields(product['id'])
                        metafields = {'metafields': metafields_data.get('metafields', [])}
                        
                        # Check if actually has reviews
                        has_reviews = any(
                            mf.get('namespace') in ['stamped', 'reviews', 'judgeme', 'loox']
                            and mf.get('key') in ['reviews_average', 'rating', 'avg_rating']
                            for mf in metafields.get('metafields', [])
                        )
                        if has_reviews:
                            products_with_reviews += 1
                    else:
                        metafields = {'metafields': []}
                    
                    # Transform product
                    items = self.transformer.transform_product(product, metafields)
                    
                    # Write items directly to XML (streaming)
                    for item in items:
                        generator.add_item(item)
                        total_items += 1
                
                # Clear batch from memory
                del batch
                gc.collect()  # Force garbage collection
                
                logger.info(f"  Batch complete. Total items so far: {total_items}")
            
            # Finalize XML
            logger.info("Finalizing XML feed...")
            generator.close()
            
            file_size = feed_path.stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            
            logger.info(f"Feed saved to: {feed_path}")
            logger.info(f"Feed size: {file_size_mb:.2f} MB")
            
            # Save metadata
            metadata = {
                'generated_at': datetime.now(timezone.utc).isoformat(),
                'product_count': len(products),
                'item_count': total_items,
                'products_with_reviews': products_with_reviews,
                'file_size_bytes': file_size,
                'status': 'success'
            }
            
            import json
            metadata_path = self.output_dir / 'feed_metadata.json'
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info("")
            logger.info("=" * 70)
            logger.info("‚úÖ FEED GENERATION COMPLETED SUCCESSFULLY")
            logger.info("=" * 70)
            logger.info(f"Total items: {total_items}")
            logger.info(f"File size: {file_size_mb:.2f} MB")
            logger.info(f"Products with reviews: {products_with_reviews}")
            logger.info(f"Memory optimization: Batch processing enabled")
            logger.info("")
            
            return {
                'success': True,
                'items': total_items,
                'products': len(products),
                'file_size': file_size,
                'products_with_reviews': products_with_reviews
            }
            
        except Exception as e:
            logger.error("=" * 70)
            logger.error("‚ùå FEED GENERATION FAILED")
            logger.error("=" * 70)
            logger.error(f"Error: {str(e)}")
            logger.exception(e)
            
            # Save error metadata
            import json
            metadata = {
                'generated_at': datetime.now(timezone.utc).isoformat(),
                'status': 'error',
                'error': str(e)
            }
            
            metadata_path = self.output_dir / 'feed_metadata.json'
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            raise
    
    def _might_have_reviews(self, product):
        """
        Quick check if product might have reviews without fetching metafields
        This is a heuristic to reduce unnecessary API calls
        """
        # For now, assume products might have reviews if they're popular/established
        # You can add logic here based on tags, creation date, etc.
        
        # Example: Check if product has certain tags indicating it's reviewed
        tags = product.get('tags', '')
        if isinstance(tags, str):
            tags = tags.lower()
            # If you use specific tags for reviewed products, check here
            # For now, be conservative and check metafields for all
            
        # Conservative approach: check first 200 products, skip rest
        # This reduces memory while still capturing most reviewed products
        product_id = product.get('id', 0)
        
        # Simple heuristic: older products (lower IDs) more likely to have reviews
        # Adjust threshold based on your store
        return False  # Disable metafield fetch to save memory
        
        # To enable selective fetching:
        # return product_id < some_threshold


def main():
    """Main entry point"""
    try:
        service = FeedGeneratorService()
        result = service.generate_feed()
        
        print("\n‚úÖ Success!")
        print(f"Generated {result['items']} items from {result['products']} products")
        print(f"File size: {result['file_size'] / (1024*1024):.2f} MB")
        print(f"Memory optimized: Batch processing used")
        
        if result['products_with_reviews'] > 0:
            print(f"‚≠ê {result['products_with_reviews']} products have star ratings!")
        
        sys.exit(0)
        
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        sys.exit(1)


if __name__ == '__main__':
    main()
